<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小记 SVM]]></title>
    <url>%2F2017%2F09%2F08%2FML-SVM%2F</url>
    <content type="text"><![CDATA[支持向量机（support vector machines）SVM是一种二类分类模型。它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机，支持向量机还包括核技巧（kernal trick）这使它成为实质上的非线性分类器。 线性可分支持向量机分类超平面对应于方程$w \cdot x + b = 0 $ 当训练数据集线性可分的时候，存在无穷个分离超平面将两类数据正确分开。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 函数间隔一般的，一个点距离分类超平面远近可以表示分类分类预测的确信程度。在分类超平面$w \cdot x + b = 0 $确定的情况下，$|w \cdot x + b|$ 能够相对的表示点$x$距离分类超平面的远近。$w \cdot x + b $的符号与标记$y$的符号是否一致能够表示分类是否预测正确。因此可以用 $y(w \cdot x + b)$来表示分类的正确性及确信度，这就是函数间隔（function margin） 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的函数间隔为$$\hat{\gamma_i} = y_i(w \cdot x_i + b)$$定义超平面$(w,b)$关于训练集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的函数间隔之最小值，$$\hat{\gamma} = \min_{i=1,\cdots,N} \hat{\gamma_i}$$ 几何间隔函数间隔可以表示分类的正确性与确信度，但是如果在选择超平面的时候成比例的改变$w$，$b$，超平面并没有发生改变，函数间隔却发生了变化。这个时候，就需要我们对超平面的法向量$w$加以约束，使得间隔是确定的，这是的函数间隔就变成了几何间隔（geometric margin） 其中$||w||$为$w$的$L2$范数 对于给定的训练数据集$T$和超平面$(w,b)$，定义超平面$(w,b)$关于样本点$(x_i,y_i)$的几何间隔$${\gamma_i} = y_i \lgroup \frac{w}{|| w ||} \cdot x_i + \frac{b}{|| w ||} \rgroup$$其中$|w|$为$w$的$L2$范数 定义超平面$(w,b)$关于训练集$T$的函数间隔为超平面$(w,b)$关于$T$中所有样本点$(x_i,y_i)$的几何间隔之最小值，$$\gamma = \min_{i=1,\cdots,N}\gamma_i$$ 因此函数间隔$\hat{\gamma}$跟几何间隔$\gamma$的关系为$$\gamma_i = \frac{\hat{\gamma_i}}{|| w ||}$$ $$\gamma = \frac{\hat{\gamma}}{|| w ||}$$ 最大间隔分离超平面（硬间隔最大化）求解一个几何间隔的最大分离超平面，可以表示为下面的最优化问题：$$\max_{w,b} \quad \gamma$$ $$s.t. \quad y_i \lgroup \frac{w}{|| w ||} \cdot x_i + \frac{b}{|| w ||} \rgroup \ge \gamma ,\quad i = 1,2,\cdots,N$$ 将函数间隔$\hat{\gamma}$跟几何间隔$\gamma$的关系带入上述最优化问题，可以将上述问题转化为$$\max_{w,b} \frac{\hat{\gamma}}{||w||}$$ $$s.t. \quad y_i(w \cdot x_i + b) \ge \hat{\gamma},\quad i=1,2,\cdots,N$$ 函数间隔 $\hat{\gamma}$的取值不会影响到上述最优化问题的求解，因此我们取$\hat{\gamma} = 1$，同时注意，最大化$\frac{1}{||w||}$和最小化$\frac{1}{2}||w||^2$是等价的，于是可以继续将上式改写为$$\min_{w,b} \frac{1}{2} ||w||^2$$ $$s.t. \quad y_i(w \cdot x_i + b ) \ge 1 ,\quad i=1,2,\cdots,N$$ 对偶问题上述的最优化问题为线性可分支持向量机的原始最优化问题，直接求解不是很容易，我们利用拉格朗日对偶性，通过求解对偶问题来间接求得原始问题的最优解，同时可以很自认的引入核函数（Kernel Function），进而引出后续的非线性分类问题。 引入拉格朗日乘子$\alpha_i$建立线性可分支持向量机的拉格朗日函数， $$L(w,b,\alpha) = \frac{1}{2}|| w ||^2 + \sum^N_{i=1}\alpha_i(1-y_i(w\cdot x_i+b))$$其中$\alpha=(\alpha_1;\alpha_2;\cdots;N)$ 将拉格朗日函数分别对$w$，$b$求偏导$$\nabla_wL(w,b,\alpha) = w - \sum_{i=1}^N \alpha_i x_i y_i = 0$$ $$\nabla_b L(w,b,\alpha) = \sum^N_{i=1} \alpha_i y_i = 0$$ 得$$w = \sum_{i=1}^N\alpha_iy_ix_i$$ $$0=\sum^N_{i=1}\alpha_iy_i$$ 带入拉格朗日函数得 $$\begin{align}L(w,b,\alpha) &amp; = \frac{1}{2} \sum_{i=1}^N \sum_{j =1 }^N \alpha_i \alpha_j y_i y_j x_i x_j + \sum_{i=1}^N \alpha_i (1 - y_i(x_i \sum_{j=1} ^ N \alpha_j y_j x_j+ b )) \\\\ &amp; = -\frac{1}{2} \sum^N_{i=1} \sum^N_{j=1} \alpha_i \alpha_j y_i y_j x_i x_j + \sum_{i=1}^N \alpha_i\end{align}$$ $$s.t. \quad \sum^N_{i=1} \alpha_i y_i = 0,\quad \alpha_i \ge 0,\quad i=1,2,\cdots,N$$ SMO算法SMO表示序列最小优化（Sequential Minimal Optimization）。SMO算法是将大优化问题分解为许多小优化问题来求解。SMO算法的目标是求出一系列$\alpha$跟$b$，一旦求出了$\alpha$，就很容易计算出权重向量$w$。 支持向量在线性可分的情况下，训练数据集中的样本点中与分离超平面距离最近的样本点构成了支持向量（support vector）。支持向量之间的距离成为间隔（margin）。间隔的大小取决于分离超平面的法向量$w$ 上图来自周志华老师《机器学习》一书中的 图6.2 支持向量与间隔 在决定分离超平面的位置的时候，其实只有支持向量在起作用，其他的训练样本点并不起作用。如果移动或者去掉了间隔边界之外的训练样本点，不会对分离超平面的位置产生影响，支持向量的个数一般都很少，因此支持向量机是由很少的重要的训练样本决定的。这也是为什么支持向量机在一些分类问题上速度较快的原因，比如分类一辆汽车跟一只猫咪。但是在汽车里去细分卡车跟轿车的话，支持向量机的效果就没有那么好了。 线性支持向量机在讨论线性可分向量机的时候，我们都是假设训练样本在样本空间一定是线性可分的，一定存在一个超平面能够将两类不同的数据给完全的分隔开，但是在实际中的数据集这样的方法往往是不成立的，这样我们就需要修改硬间隔最大化时，引入了软间隔最大化。 上图来自周志华老师《机器学习》一书中的 图6.4 软间隔示意图.红色圈出了一些不满足约束条件的样本 软间隔最大化之前介绍的向量机形式都是要求所有的样本必须满足约束条件$y_i(w^Tx_i+b) \ge 1$，所有的样本都必须正确的划分，这种形式的向量机成为硬间隔（hard marigin），而软间隔（soft marigin）则允许某些样本不用满足上述的约束条件，当去除掉这些特异点后，剩下的大部分样本组成的点一定是线性可分的。 通过引入松弛变量$\xi_i \ge0$，修改硬间隔最大化的约束条件 ，得到如下的约束条件，$$s.t. \quad y_i(w\cdot x_i + b) \ge 1 - \xi_i$$同样，在最大化软间隔的同时，我们也是希望不满足上述约束条件的样本尽可能的少，对于每一个不满足硬间隔最大化约束但是满足软间隔最大化约束的特异点都引入一个loss代价$\xi_i$,这样就变成了软间隔最大化的最优化问题$$\min_{w,b,\xi} \frac{1}{2} ||w||^2 + C\sum^N_{i=1} \xi_i$$ $$s.t. \quad y_i(w\cdot x_i + b) \ge 1 - \xi_i, \quad\xi_i \ge 0,\quad i = 1,2,\cdots,N$$ 在关于软间隔最大化另一种表达形式中，我们用$\ell_{0/1}$损失代替上式中引入松弛变量$\xi_i$带来的损失$\xi_i$,因此优化函数可以写为$$\min _{w,b}\frac{1}{2} ||w||^2 + C\sum^m_{i=1} \ell_{0/1}(y_i(w\cdot x_i+b)-1)$$其中的$C &gt; 0​$是一个常数 ，$\ell_{0/1}​$是0/1损失函数$$\begin{equation}\ell_{0/1}(z) =\left \{\begin{array} {r@{\quad:\quad}l} 1 &amp; z &lt; 0\\ 0 &amp; else\end{array}\right.\end{equation}$$由于$\ell_{0/1}$函数是不连续的，因此可以通过其他的连续函数来引入到最大软间隔的最优化问题中，下面举几个常用的例子。 折页损失（hinge loss） $$\ell_{hinge}(z) = \max(0,1-z)$$ 若采用hinge loss，则二分类问题的线性支持向量机的优化函数可以写为$$\min _{w,b}\frac{1}{2} ||w||^2 + C\sum^m_{i=1}\max(0,1-y_i(w \cdot x_i + b))$$ 对于多分类问题，所求的分类器是一个$k$类的线性分类器。 对于$s = Wx_i + b$，此时的$W \in \mathbb{R} ^{k \times n}$，为一个$k \times n$矩阵，$b \in \mathbb{R} ^{k }$，为一个向量，因此有$s \in \mathbb{R} ^{k }$，$s$中的每个分量表示分类器在该类别中的得分，样本$x_i$的标签$y_i \in \mathbb{R} ^{k }$，若$x_i$属于类别$k$，则$y_i$中除了第$k$个分量外其余分量全部为0。$s_j$表示得分向量s中的第$j$个分量，$s_{y_i}$表示对应$y_i =1$的分量。 单个样本多分类的Hinge Loss 可以表示为$$\sum_{j \ne y_i} \max (0,s_j-s_{y_i} + 1)$$$k$分类线性分类SVM的Hinge Loss表示为$$\min_{W,b} \sum _{i=1}^N \sum _{j \ne y_i} \max(0,s_j - s_{y_i} + 1)$$引入正则项得$$\min_{W,b} \sum _{i=1}^N \sum _{j \ne y_i} \max(0,s_j - s_{y_i} + 1) + \lambda \sum_k \sum_n W^2_{k,n}$$ 指数损失（exponential loss） $$\ell_{exp}(z) = \exp(-z)$$ 对率损失（logistic loss ） $$\ell_{log}(z) = log(1+\exp(-z))$$ 上图来自周志华老师《机器学习》一书中的 图6.5 三种常见的替代损失函数：hinge损失，指数损失，对率损失。 非线性支持向量机核函数在上面的线性支持向量机的讨论中，假设训练样本的数据集都是线性可分的，存在一个划分超平面可以将数据集给正确分类。 对于非线性的问题往往不好求解，我们还是希望能够转化为线性分类问题来求解，通过将样本从原始空间映射到一个更高的特征空间上，使得训练样本在高维的特征空间上线性可分。 上图来自周志华老师《机器学习》一书中的 图6.3 异或问题与非线性映射 幸运的是，如果原始空间是有限维的，那么一定存在一个高维特征空间使得样本可分。 令$\phi(x)$为将$x$映射后的特征向量，因此在高维特征空间中划分超平面的模型函数为$$f(x) = w^T\phi(x) + b$$其中$w$，$b$为参数模型。类比一般情况下的支持向量，最大间隔分离超平面的模型为$$\min_{x,b} \frac{1}{2} ||w||^2$$ $$s.t. \quad y_i(w^T\phi(x_i) + b) \ge 1,\quad i=1,2,\cdots,m$$ 其对偶问题是$$\max_{\alpha} \sum^{m}_{i=1}\alpha_i - \frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i \alpha_j y_i y_j\phi(x_i)^T\phi(x_j)$$ $$s.t. \quad \sum^m_{i=1} \alpha_i y_i = 0，\quad \alpha_i \ge0,\quad i =1,2,\cdots,m$$ 求解上式涉及到$\phi(x_i)^T\phi(x_j)$的计算，这是样本$x_i$与$x_j$映射到特征空间之后的内积，特征空间的维数很高，直接求解上式的难度很大，为了避开这个障碍，我们可以设想这样的一个函数$$\kappa(x_i,x_j) = \langle\phi(x_i),\phi(x_j)\rangle = \phi(x_i)^T\phi(x_j)$$使得$x_i$与$x_j$在特征空间的内积等于它们在原始空间上通过函数$\kappa(\cdot,\cdot)$来计算，这里的函数$\kappa(\cdot,\cdot)$就是核函数（kernel function）。将核函数带入最大分隔超平面模型后，$$\max_{\alpha} \sum^{m}_{i=1}\alpha_i - \frac{1}{2}\sum^m_{i=1}\sum^m_{j=1}\alpha_i \alpha_j y_i y_j \kappa(x_i,x_j)$$ $$s.t. \quad \sum^m_{i=1} \alpha_i y_i = 0，\quad \alpha_i \ge0,\quad i =1,2,\cdots,m$$ 求解后，可以得到$$\begin{align}f(x) &amp;= w^T\phi(x) + b \\\\&amp;= \sum_{i=1}^m\alpha_iy_i\phi(x_i)^T\phi(x) +b \\\\ &amp; = \sum_{i=1}^m \alpha_i y_i \kappa(x,x_i) + b\end{align}$$ 几种常见的核函数 线性核 $$\kappa(x_i,x_j) = x_i^Tx_j$$ 多项式核 $$\kappa(x_i,x_j) =(x_i^T x_j)^d$$ 高斯核 $$\kappa(x_i,x_j) = \exp (-\frac{||x_i-x_j||^2}{2\sigma^2})$$ 拉普拉斯核 $$\kappa(x_i,x_j) = \exp (-\frac{||x_i-x_j||}{\sigma})$$ Sigmoid核 $$\kappa(x_i,x_j) = \tanh(\beta x_i^T x_j + \theta)$$ 常见核函数的组合也是核函数 若$\kappa_1$和$\kappa_2$为核函数，对于任意正数$\gamma_1$、$\gamma_2$，其线性组合$\gamma_1 \kappa_1 + \gamma_2 \kappa_2$也是核函数。 若$\kappa_1$和$\kappa_2$为核函数，则核函数的直积$\kappa_1\otimes \kappa_2(x,z) = \kappa_1(x,z)\kappa_2(x,z)$也是核函数。 若$\kappa_1$为核函数，对于任意函数$g(x)$，$\kappa(x,z) = g(x)\kappa_1(x,z)g(z)$也是核函数。 参考文献 《机器学习》周志华老师著 《统计学习方法》李航老师著 CS231n Convolution Neual Networks for Visual Recognition]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Gradient Descent]]></title>
    <url>%2F2017%2F09%2F02%2FML-Gradient-Descent%2F</url>
    <content type="text"><![CDATA[Gradient Descent（梯度下降法）是一个一阶最优化算法，又被称为最速下降法。通过Gradient Descent算法对函数上当前点对应梯度的反方向的指定步长进行迭代搜索，我们可以很快的找到一个Local Minimum（局部极小值）。如果是对当前点对应梯度的正方向进行迭代搜索，我们可以求得一个Local Maximum（局部最大值）。 在应用到机器学习算法的时候，我们通常采用Gradient Descent来对所采用的算法进行训练，使得loss函数的目标值达到最优。 以前面提到过的Linear Regression为例，假设函数为$$h_\theta(x) = \theta^Tx = \sum_{j=0}^n\theta_j x_j$$对应的损失函数loss为$$J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$$ 其中，$n$为训练样本的特征数量；$m$为训练样本数。 Batch Gradient DescentBatch Gradient Descent（批量梯度下降法，BGD）是 Gradient Descent（梯度下降法）的最基本形式，算法主要布置是：在更新每个参数的时候，都要使用所有的样本来进行更新。 对上述的损失函数loss求偏导： $$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x^{(i)}_j$$ $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta) },\quad ( j = 0,1,\cdots,n)$$ Stochastic Gradient DescentStochastic Gradient Descent（随机梯度下降法，SGD） 由于Batch Gradient Descent（批量低度下降法，BGD）在更新每一个参数$\theta_j$时，都需要计算到所有训练样本，因此在训练的过程会随着样本数量的增大而变的异常缓慢。针对这个缺点，提出了Stochastic Gradient Descent（随机梯度下降法，SGD）。$$\begin{align} J(\theta) &amp;= \frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 \\ \\ &amp;= \frac{1}{m} \sum_{i=1}^m \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2 \\\\ &amp;=\frac{1}{m} \sum_{i=1}^m cost(\theta,(x^{(i)},y^{(i)})) \\\end{align}$$即$cost(\theta,(x^{(i)},y^{(i)})) = \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2$ 利用每个样本的loss函数对$\theta$求偏导得到对应的梯度，来更新$\theta$$$\begin{align}Repeat \ \{ \\\\ for \ \ i &amp;= 1，2，\cdots ，m \ \{ \\\\ \theta_j &amp;= \theta_j - \alpha (h_\theta(x^{(i)}) - y^{(i)})x^i_j \\\\ (j &amp; = 0，1，\cdots，n) \\\\ &amp; \quad \} \\\\ \}\end{align}$$ Stochastic Gradient Descent 是通过每个样本来迭代更新一次，在遇到样本数量$m$很大的时候，有可能仅仅通过其中的几万条或者几千条的样本就可以将$\theta$给迭代到最优了，对比Batch Gradient Descent每迭代一次都要用到所有的训练样本，效率上会有很大的提升。 但是Stochastic Gradient Descent伴随的一个问题是训练噪音比Batch Gradient Descent要多，使得每次的Stochastic Gradient Descent迭代并不是都朝着全局最优的方向优化。 比较下Batch Gradient Descent跟Stochastic Gradient Descent，先给出Batch Gradient Descent的表达式$$x_{t+1} = x_t - \eta_t \nabla f(x_t)$$同样的，给出Stochastic Gradient Descent的表达式定义$$x_{t+1} = x_t - \eta_t g_t$$其中$g_t$就是Stochastic Gradient，它满足$E(g_t) = \nabla f(x_t)$，也就是说虽然Stochastic Gradient有一定的随机性，但是从数学期望上来说，它是等于正确的导数的，虽然每一次的迭代并不是朝着全局最优的方向，但是在大的整体上是朝着全局最优方向的，最终的结果也往往在全局最优解附近。 虽然说Stochastic Gradient Descent会伴随的随机噪声的问题，但是在实际表现中，Stochastic Gradient Descent的表现要Batch Gradient Descent的表现要好的多，加了噪声的算法表现反而更好。这一点主要取决于Stochastic Gradient Descent的一些不错的性质，它能够自动逃离鞍点，自动逃离比较差的局部最优点。 关于具体的一些证明可以看一下参考文献部分第三个条目。 Mini-batch Gradient DescentMini-batch Gradient Descent（小批量梯度下降法，MBGD） Method The difference Batch Gradient Descent Use all $m$ examples in each iteration Stochastic Gradient Descent Use one example in each iteration Mini-batch Gradient Descent Use $b$ examples in each iteration $$\begin{align}\theta_j &amp;= \theta_j - \alpha \frac{1}{b} \sum_{k=i}^{i+b-1}(h_\theta(x^{(k)})-y^{(k)})x_j^{(k)} \\\\ &amp;s.t. \quad i = i + b\end{align}$$ 参考文献 Couresera Machine Learning Andrew-Ng Large Scale Machine Learning Stochastic Gradient Descent 为什么说随机最速下降法(SGD)是一个很好的方法？ BGD，SGD，MBGD的对比]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Neural Network]]></title>
    <url>%2F2017%2F05%2F16%2FML-Neural-Network%2F</url>
    <content type="text"><![CDATA[人工神经网络在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN），是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型，用于对函数进行估计或近似。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统现代神经网络是一种非线性统计性数据建模工具。典型的神经网络具有以下三个部分： 结构 （Architecture）：结构指定了网络中的变量和它们的拓扑关系。例如，神经网络中的变量可以是神经元连接的权重（weights）和神经元的激励值（activities of the neurons）。 激励函数（Activity Rule）：大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。一般激励函数依赖于网络中的权重（即该网络的参数）。 学习规则（Learning Rule）：学习规则指定了网络中的权重如何随着时间推进而调整。这一般被看做是一种长时间尺度的动力学规则。一般情况下，学习规则依赖于神经元的激励值。它也可能依赖于监督者提供的目标值和当前权重的值。 神经网络结构神经网络被建模成神经元的集合，神经元之间以无环图的形式进行连接，也就是说，一些神经元的输出是另一些神经元的输入。在网络中是不允许循环的，这样的循环会导致前项传播的无限循环。通常神经网络中神经元是分层的，而不像生物神经元一样聚合成大小不一的聚合状，最常见的层的类型是全连接层（fully-connected layer）。全连接层中的神经元跟其前后两层的神经元是完全连接的，但是在同一层神经元之间是没有连接的。下图是一个3层神经网络。（不包括输入层）。 神经网络的常用激励函数每个激活函数（或非线性函数）的输入都是一个数字，然后对其进行某种固定的数学操作。 sigmoid sigmoid非线性激活函数的数学公式$\sigma(x) = \frac{1}{1+e^{-x}}$，正如在logistics regression中，它输入实数并将其映射到0到1的范围内。具体的说是很大的正数变成1，很大的负数变成0。 softmax softmax函数又称为归一化指数函数，他将一个含任意实数的$k$维向量$z$映射到另外一个$k$维实向量$\sigma(z)$中，使得每一个元素的范围都在$(0,1)$之间，并且所有元素的和为1$$\begin{align}\sigma(z)_j =&amp; \frac {\exp (z_j)}{\sum^K_{k=1}exp(z_k)} \\\\ j =&amp; 1,2\cdots,K\end{align}$$ $$\begin{align}h_\theta(x) =\begin{bmatrix}P(y = 1 | x; \theta) \\P(y = 2 | x; \theta) \\\vdots \\P(y = K | x; \theta)\end{bmatrix}=\frac{1}{ \sum_{j=1}^{K}{\exp(\theta^{(j)\top} x) }}\begin{bmatrix}\exp(\theta^{(1)\top} x ) \\\exp(\theta^{(2)\top} x ) \\\vdots \\\exp(\theta^{(K)\top} x ) \\\end{bmatrix}\end{align}$$ $$\theta = \left[\begin{array}{cccc}| &amp; | &amp; | &amp; | \\\theta^{(1)} &amp; \theta^{(2)} &amp; \cdots &amp; \theta^{(K)} \\| &amp; | &amp; | &amp; |\end{array}\right].$$ softmax函数实际上是有限项离散概率分布的梯度对数归一化。 Softmax Regression 有一个不寻常的特点：它有一个“冗余”的参数集。假设我们在$\theta^{(k)}$中减去$\psi$时，不影响原函数的预测结果。这表明前面的 softmax 回归模型中存在冗余的参数， Softmax 模型被过度参数化了overparameterized。对于任意一个用于拟合数据的假设函数，可以求出多组参数值，这些参数得到的是完全相同的假设函数 $h_\theta(x)$。如下所示，$$\begin{split}P(y^{(i)} = k | x^{(i)} ; \theta)&amp;= \frac{\exp((\theta^{(k)}-\psi)^\top x^{(i)})}{\sum_{j=1}^K \exp( (\theta^{(j)}-\psi)^\top x^{(i)})} \\&amp;= \frac{\exp(\theta^{(k)\top} x^{(i)}) \exp(-\psi^\top x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)}) \exp(-\psi^\top x^{(i)})} \\&amp;= \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}.\end{split}$$ Tanh tanh非线性函数的图像如下图所示，它讲实数值压缩到[-1,1]之间。与sigmoid神经元不同的是，它的输出是零中心的，因此在实际的操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。tanh函数是一个简单放大的sigmoid神经元，$tanh(x) = 2\sigma(2x) -1$。 ReLU ReLU函数的公式$f(x) = max (0,x)$，这个激活函数就是一个关于0的阈值，见下图。 优点：相较于sigmoid跟tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用，sigmoid跟tanh神经元含有指数运算等耗费计算资源的操作，ReLU可以简单的通过对一个矩阵进行阈值计算得到。 缺点：ReLU单元在训练的时候有可能“死掉”。当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新后无法被其他任何数据点再次激活。从此所以流过这个神经元的梯度将都变成0。也就是说，这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。例如，如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。 Leaky ReLU Leaky ReLU是为了解决”ReLU死亡”问题的，ReLU中，$x&lt;0$时，函数值为0， 而在Leaky ReLU中则是一个很小的负数梯度值，比如0.01。 Maxout todo Relationship of Logistic Rregression and Softmax Regression当k = 2 时，Softmax Regression可以写为$$\begin{align}h_\theta(x^{(i)}) &amp;=\frac{1}{ \exp(\theta^{(1)^{\top}}x^{(i)}) + \exp( \theta^{(2)^{\top}} x ^{(i)}) }\begin{bmatrix}\exp( \theta^{(1)^{\top}} x ^{(i)}) \\\exp( \theta^{(2)^{\top}} x ^{(i)})\end{bmatrix}\end{align}$$我们令$\psi = \theta^{(1)}​$ 并且在两个参数向量中都减去向量$\theta^{(1)}​$，得到$$\begin{split}h_\theta(x^{(i)}) &amp;=\frac{1}{ \exp(\vec{0}^{\top}x^{(i)}) + \exp(( {\theta^{(2)}-\theta^{(1)}})^{\top} x ^{(i)})}\begin{bmatrix}\exp( \vec{0}^{\top} x^{(i)} ) \\\exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x^{(i)} )\end{bmatrix}\\\\&amp;=\begin{bmatrix}\frac{1}{ 1 + \exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x^{(i)} ) } \\\frac{\exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x^{(i)} )}{ 1 + \exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x ^{(i)}) }\end{bmatrix} \\\\&amp;=\begin{bmatrix}\frac{1}{ 1 + \exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x^{(i)} ) } \\1 - \frac{1}{ 1 +\exp( ( {\theta^{(2)}-\theta^{(1)}})^{\top} x^{(i)} ) } \\\end{bmatrix}\end{split}$$用$\theta^{‘}​$表示$\theta^{(1)}-\theta^{(2)}​$，我们会发现Softmax Regression 预测其中的一个类别的概率为$\frac{1}{ 1 + \exp(- (\theta’)^\top x^{(i)} ) }​$ ，另一个类别的概率为$1-\frac{1}{ 1 + \exp(- (\theta’)^\top x^{(i)} ) }​$，这就是Logistics Regression 。 代价函数logistic regression cost function$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)}) ) +(1-y^{(i)})\log(1-h_\theta(x^{(i)}))$$ neural network神经网络模型的代价函数取决于输出层是什么，对于不同的场景需要，对应于不同的代价函数。例如，在Autoencoder网络中，输出层等于输入层，此时采用均方误差（MSE）函数作为代价函数；在分类问题中，如果输出层采用Softmax回归进行分类，则可以直接采用Softmax回归的代价函数作为整个神经网络的代价函数。如果输出层采用Logistic regression进行分类，那么输出层其实就是K个Logistic regression，整个网络的代价函数就是这K个Logistic regression模型代价函数的加和。 输出层采用Logistic Regression$$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k) \Bigg]$$ 输出层采用 Softmax Regression$$\begin{align}J(\theta) = - \frac{1}{m}\left[ \sum_{i=1}^{m} \sum_{k=1}^{K} 1\left\{y^{(i)} = k\right\} \log \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)})}\right]\end{align}$$ $$\begin{align}\nabla_{\theta^{(k)}} J(\theta) = - \frac{1}{m}\sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = k\} - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right] }\end{align}$$ $$P(y^{(i)} = k | x^{(i)} ; \theta) = \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)}) }$$ Autoencoder（输出层=输入层） todo 对比Logistics Regression 跟 Softmax Regression，在Logistics Regression 中$$P(y^{(i)} = k | x^{(i)} ; \theta) = y_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k$$在Softmax Regression中，$P(y^{(i)} = k | x^{(i)} ; \theta) = \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top} x^{(i)}) }$，因此Logtistic Regression 也可以写为以下的形式：$$\begin{align}J(\theta) &amp;= - \left[ \sum_{i=1}^m (1-y^{(i)}) \log (1-h_\theta(x^{(i)})) + y^{(i)} \log h_\theta(x^{(i)}) \right] \\&amp;= - \left[ \sum_{i=1}^{m} \sum_{k=0}^{1} 1\left\{y^{(i)} = k\right\} \log P(y^{(i)} = k | x^{(i)} ; \theta) \right]\end{align}$$ logistic regression cost function regularization$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)}) ) +(1-y^{(i)})\log(1-h_\theta(x^{(i)})) + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$$ neural network regularization 输出层采用Logistic Regression$$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\Bigg] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2$$ 输出层采用 Softmax Regression$$J(\theta) = -\left [ \sum_{i=1}^{m} \sum_{k=1}^{K} 1\left\{y^{(i)} = k\right\} \log \frac{\exp(\theta^{(k)\top} x^{(i)})}{\sum_{j=1}^K \exp(\theta^{(j)\top}x^{(i)})}\right]+\frac{\lambda}{2}\sum^K_{k=1}\sum^n_{j=1}\theta^2_{kj}$$ $$\nabla_{\theta^{(k)}} J(\theta) = - \frac{1}{m} \sum_{i=1}^{m}{ \left[ x^{(i)} \left( 1\{ y^{(i)} = k\} - P(y^{(i)} = k | x^{(i)}; \theta) \right) \right] } + \lambda\theta^{(k)}$$ Autoencoder（输出层=输入层） todo 下面我们用输出层采样为Logistics Regression 为例子来说明。 Feed forward and Back propagation一些标记: L 表示神经网络的总层数 $S_l$表示第$l$层神经网络unit个数，不包括偏差单元bias unit k表示第几个输出单元 $\Theta^{(l)}_{i,j}$ 第$l$层到第$l+1$层的权值矩阵的第$i$行第$j$列的分量 $Z^{(j)}_i$ 第$j$层第$i$个神经元的输入值 $a^{(j)}_i$第$j$层第$i$个神经元的输出值 $a^{(j)} = g(Z^{(j)})$ Feed forward computation $h_\theta(x^{(i)})$ 1234567891011% computation h(x)% input layerxa1 = [ones(m,1) X];% hidden layerZ2 = a1*Theta1';a2 = sigmoid(Z2);a2 = [ones(size(a2,1),1) a2];% output layerZ3 = a2*Theta2';a3 = sigmoid(Z3);h = a3; $$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k) \Bigg]$$ 1234567891011121314151617181920%case 1J = 0;Y = zeros(m,num_labels);for i = 1 : m Y(i,y(i)) = 1;endJ = -1/m * (Y * log(h)' + (1 - Y) * log(1 - h)');J = trace(J);%case 2J = 0;Y = zeros(m,num_labels);for i = 1 : m Y(i,y(i)) = 1;endfor i = 1 : m J = J + -1*m *(Y(i,:) * log(h(i,:))' + (1 - Y(i,:)* log(1 - h(i,:))');end Chain Rule$y = g(x)$ $ z = h(y)$ $\Delta x \rightarrow \Delta y \rightarrow \Delta z $ $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$ $x = g(s) $ $y = h(s)$ $z = k(x,y)$ $\frac{dz}{ds} = \frac{\partial z}{\partial x} \frac{dx}{ds} + \frac{\partial z }{\partial y} \frac{dy}{ds}$ back propagation我们知道代价函数cost function后，下一步就是按照梯度下降法来计算$\theta$求解cost function的最优解。使用梯度下降法首先要求出梯度，即偏导项$\frac{\partial}{\partial \Theta^{(l)} _{ij}} J(\Theta)$，计算偏导项的过程我们称为back propagation。 根据上面的feed forward computation 我们已经计算得到了 $a^{(1)}$ ，$a^{(2)}$， $a^{(3)}$ ，$Z^{(2)}$，$Z^{(3)}$。 hidden layer to output layer$$h_\Theta(x) = a^{(L)} = g(z^{(L)})$$ $$z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$$ $$\frac{\partial}{\partial \Theta^{(L-1)}_{i,j}}J(\Theta) = \frac {\partial J(\Theta)}{\partial h_\theta(x)_i} \frac{\partial h_\theta(x)_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} = \frac {\partial J(\Theta)}{\partial a^{(L)}_i}\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}}$$ $$cost(\Theta) =- y^{(i)}\log(h_\Theta(x^{(i)}) ) -(1-y^{(i)})\log(1-h_\Theta(x^{(i)}))$$ $$\frac{\partial J(\Theta)}{\partial a^{(L)}_i} =\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i}$$ 由下式得$$\begin{split}\frac{\partial g(z)}{\partial z} &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2\frac{\partial{}}{\partial{z}} \left(1 + e^{-z} \right) \\\\ &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2e^{-z}\left(-1\right) \\\\ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{1}{1 + e^{-z}} \right)\left(e^{-z}\right) \\\\ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{e^{-z}}{1 + e^{-z}} \right) \\\\ &amp; = \left( \frac{1}{1+e^{-z}}\right)\left( \frac{1+e^{-z}}{1+e^{-z}}-\frac{1}{1+e^{-z}}\right) \\\\ &amp; = g(z) \left( 1 - g(z)\right) \\\\ \end{split}$$ $$\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} = \frac{\partial g(z^{(L)}_i)}{\partial z^{(L)}_i} = g(z^{(L)}_i)(1- g(z^{(L)}_i))=a^{(L)}_i(1- a^{(L)}_i)$$ $$\frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} = a^{(L-1)}_j$$ 综上$$\begin{split}\\ \frac{\partial}{\partial \Theta^{(L-1)}_{i,j}}J(\Theta) &amp;= \frac {\partial J(\Theta)}{\partial a^{(L)}_i}\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} \\\\ &amp;=\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i} a^{(L)}_i(1- a^{(L)}_i) a^{(L-1)}_j \\\\ &amp;= (a^{(L)}_i - y_i) a_j^{(L-1)}\end{split}$$ hidden layer / input layer to hidden layer因为$a^{(1)} = x$，所以可以将 input layer 与 hidden layer同样对待$$\frac{\partial}{\partial \Theta^{(l-1)}_{i,j}}J(\Theta) = \frac {\partial J(\Theta)}{\partial a^{(l)}_i} \frac{\partial a^{(l)}_i}{\partial z^{(l)}_i}\frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \ (l = 2,3, …, L-1)$$ $$\frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} =\frac{\partial g(z^{(l)}_i)}{\partial z^{(l)}_i} =g(z^{(l)}_i)(1- g(z^{(l)}_i))=a^{(l)}_i(1- a^{(l)}_i)$$ $$\frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} = a^{(l-1)}_j$$ 第一部分的偏导比较麻烦，要使用chain rule。$$\dfrac{\partial J(\Theta)}{\partial a_i^{(l)}}= \sum_{k=1}^{s_{l+1}} \Bigg[\dfrac{\partial J(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]$$ $$\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} = a^{(l+1)}_k (1 - a^{(l+1)}_k)$$ $$\frac{\partial z^{(l+1)}_k}{\partial a^{(l)}_i} = \Theta^{(l)}_{k,i}$$ 求得递推式为：$$\begin{split}\\ \frac{\partial J(\Theta)}{\partial a^{(l)}_i} &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\dfrac{\partial J(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]\\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\frac{\partial J(\Theta)}{\partial a^{(l+1)}_k}\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} \Theta^{(l)}_{k,i} \Bigg] \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[ \frac{\partial J(\Theta)}{\partial a^{(l+1)}_k}a^{(l+1)}_k (1 - a^{(l+1)}_k) \Theta^{(l)}_{k,i} \Bigg]\end{split}$$定义第$l$层第$i$个节点的误差为：$$\begin{split}\delta^{(l)}_i &amp;= \frac{\partial}{\partial z^{(l)}_i} J(\Theta)\\&amp;= \frac{\partial J(\Theta)}{\partial a^{(l)}_i} \frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} \\\\ &amp;=\frac{\partial J(\Theta)}{\partial a^{(l)}_i} a^{(l)}_i (1 - a^{(l)}_i) \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\frac{\partial J(\Theta)}{\partial a^{(l+1)}_k}\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i) \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\delta^{(l+1)}_k \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i) \\end{split}$$ $$\begin{split}\delta^{(L)}_i &amp;= \frac{\partial J(\Theta)}{\partial z^{(L)}_i} \\\\ &amp;= \frac {\partial J(\Theta)}{\partial a^{(L)}_i} \frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \\\\ &amp;=\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i} a^{(L)}_i(1- a^{(L)}_i) \\\\ &amp;= a^{(L)}_i - y_i\end{split}$$ 最终代价函数的偏导数为$$\begin{split}\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta) &amp;= \frac {\partial J(\Theta)}{\partial a^{(l)}_i}\frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\&amp;= \frac {\partial J(\Theta)}{\partial z^{(l)}_i} \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\ &amp;= \delta^{(l)}_i \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\ &amp;= \delta^{(l)}_i a^{(l-1)}_j\end{split}$$ 总结 输出层的误差 $\delta^{(L)}_i$$$\delta^{(L)}_i = a^{(L)}_i - y_i$$ 隐层误差 $\delta^{(l)}_i$$$\delta^{(l)}_i =\sum_{k=1}^{s_{l+1}} \Bigg[\delta^{(l+1)}_k \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i)$$ 代价函数偏导项 $\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta)$$$\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta) = \delta^{(l)}_i a^{(l-1)}_j$$即$$\frac {\partial}{\partial \Theta^{(l)}_{i,j}} J(\Theta) = \delta^{(l+1)}_i a^{(l)}_j$$​ 让我们重新整下back propagation的过程。 首先，我们定义每层的误差$$\delta^{(l)} = \frac {\partial}{ \partial z^{(l)}} J(\Theta)$$$\delta^{(l)}_j$ 表示第$l$层第$j$个节点的误差。为了求出偏导项$\frac{\partial}{\partial \Theta^{(l)} _{ij}} J(\Theta)$，我们首先要求出每一层的$\delta$（不包括第一层，第一层是输入层，不存在误差），对于输出层第四层 $$\begin{align}\delta_i^{(4)} &amp; = \frac{\partial}{\partial z_i^{(4)}}J(\Theta) \\ \\&amp; = \frac{\partial J(\Theta)}{\partial a_i^{(4)}}\frac{\partial a_i^{(4)}}{\partial z_i^{(4)}} \\ \\&amp; = -\frac{\partial}{\partial a_i^{(4)}}\sum_{k=1}^K\left[y_kloga_k^{(4)}+(1-y_k)log(1-a_k^{(4)})\right]g’(z_i^{(4)}) \\ \\&amp; = -\frac{\partial}{\partial a_i^{(4)}}\left[y_iloga_i^{(4)}+(1-y_i)log(1-a_i^{(4)})\right]g(z_i^{(4)})(1-g(z_i^{(4)})) \\\\ &amp; = \left(\frac{1-y_i}{1-a_i^{(4)}}-\frac{y_i}{a_i^{(4)}}\right)a_i^{(4)}(1-a_i^{(4)}) \\\\ &amp; = (1-y_i)a_i^{(4)} - y_i(1-a_i^{(4)}) \\\\ &amp; = a_i^{(4)} - y_i \\\end{align}$$ $$\begin{split}\delta_i^{(l)} &amp; = \frac{\partial}{\partial z_i^{(l)}}J(\Theta) \\\\ &amp; = \sum_{k=1}^{S_{l+1}}\frac{\partial J(\Theta)}{\partial z_k^{(l+1)}}\frac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\frac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \\\\ &amp; = \sum_{k=1}^{S_{l+1}}\delta_k^{(l+1)}\Theta_{ki}^{(l)} g’(z_i^{(l)}) \\\\ &amp; = g’(z_i^{(l)})\sum_{k=1}^{S_{l+1}}\delta_k^{(l+1)}\Theta_{ki}^{(l)}\end{split}$$ 写成向量的形式：$$\delta^{(l)} = (\Theta^{(l)})^T\delta^{(l+1)} \ldotp\ast g’(z^{(l)})$$求出所有的$\delta$后，我们可以得到$$\frac {\partial}{\partial \Theta ^{(l)}_{i,j}} J(\Theta) = \delta^{(l+1)}_i a^{(l)}_j$$ 123456delta_3 = h - Y;delta_2 = delta_3 * Theta2 .* a2 .*(1 - a2);delta_2 = delta_2(:,2:end);Theta1_grad = delta_2' * a1 / m;Theta2_grad = delta_3' * a2 / m ​ 参考文献 《机器学习》周志华老师著 《统计学习方法》李航老师著 维基百科 softmax函数 维基百科 人工神经网络 CS231n课程翻译笔记 CS231n Convolution Neual Networks for Visual Recognition 神经网络 UFLDL Tutorial Machine Learning]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Logistic Regression]]></title>
    <url>%2F2017%2F05%2F05%2FML-Logistic-Regression%2F</url>
    <content type="text"><![CDATA[Logistic Regression 对数几率回归接下来介绍下广义线性回归，也很简单，我们不再只用线性函数来模拟数据，而是在外层添加了一个单调可微函数$g(z)$，即$f(x_i) = g(wx_i+b) $ ，如果 $ g=ln(x) $，则这个广义线性回归就变成了对数线性回归，其本质就是给原来线性变换加上了一个非线性变换，使得模拟的函数有非线性的属性。但本质上的参数还是线性的，主体是内部线性的调参。 如果我们觉得模型应该是指数变化的时候，我们可以简单粗暴的把线性模型映射到指数变化上，如上图中的红线映射到黑色的指数线，这就是广义线性模型的思想 对数几率回归（Logistic Regression）不是解决回归问题的，而是解决分类问题的。目的是要构造出一个分类器（Classifier）。对数几率回归（Logistic Regression）的关键并不在于回归，而在于对数几率函数。 对一个简单的二分类问题，实际上是样本点或者预测点到一个值域为 的函数，函数值表示这个点分在正类（postive）或者反类（negtive）的概率，如果非常可能是正类（postive），那么其概率值就逼近与1，如果非常可能是反类（negtive）其概率值就逼近与0。 构造一个sigmoid函数 $y=\frac{1}{1+ \mathrm{e}^{-z} } $。 1y = 1 ./ ( 1 + exp(-z) ); 在对数几率回归（Logistic Regression），假设函数为$h_\theta (x) = g (\theta^T x)$，其中$g(z)=\frac{1}{1+ \mathrm{e}^{-z} } $就是我们上面构造的sigmoid函数。即，$$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$$我们可以将对率函数的输出理解为，当输入为$x$的时候，$y=1$的概率，可以用$h_\theta(x) = P(y = 1 | x;\theta)$ 表达。对于sigmoid函数，当$z &gt; 0$时 ，$g(z) \geq 0.5$ 即预测 $y = 1$，当$z &lt; 0$时，$g(z) &lt; 0.5$ 即预测$ y = 0$。 Logistic Regression cost function跟线性回归中的代价函数相比，$$J(\theta) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$线性回归之所以可以使用梯度下降法来下降到最优解是因为代价函数$J(\theta)$是一个凸函数。对于对率回归而言，假设函数$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 是一个非线性的复杂模型，代价函数就不是一个凸函数(non-convex)。这样使用梯度下降法就只能得到局部最优解而非全局最优解，所以我们需要构造一个合理的并且是凸函数的代价函数。 对于$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 可以变化为$\ln\frac{y}{1-y} = \theta^Tx$，若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例的可能性，两者的比值$\frac{y}{1-y}$成为几率（odds），反映了$x$作为正例的相对可能性，对几率取对数则得到了对数几率（log odds ，亦称logit）$\ln\frac{y}{1-y}$。$$\ln\frac{p(y=1 | x)}{p(y = 0| x)} = \theta^Tx$$显然有$$p(y = 1|x) = \frac{e^{\theta^Tx}}{1+e^{\theta^Tx}} = \frac{1}{1+e^{-\theta^Tx}}$$ $$p(y = 0|x) = \frac{1}{1+e^{\theta^Tx}}$$ 于是我们可以通过极大似然法（maximum likehood method）来估计$\theta$，得到似然函数：$$L(\theta) = \prod_{i=1}^mp(y_i|x_i;\theta)$$ 对率回归模型最大化对数似然（log-likehood）$$\ell(\theta) = \sum_{i=1}^m\ln p(y_i|x_i;\theta)$$即令每个样本属于其真实标记的概率越大越好。为了方便讨论，我们令$p_1(x;\theta) = p(y = 1|x;\theta) $，$p_0(x;\theta) = p( y = 0 | x;\theta)$，则上式子中的似然项可以重写为$$p(y_i|x_i;\theta) = y_ip_1(x_i;\theta) + (1 - y_i)p_0(x_i;\theta)$$带入对率回归模型中得，$$\ell(\theta) = \sum_{i=1}^m \ln [y_i p_1(x_i;\theta) + (1 - y_i) p_0(x_i;\theta)]$$ 得到了对数似然函数，通过求对数似然函数的最大值来估计模型参数。根据已知的$p0$$p1$，可得，上述对数似然函数的最大化式等价于下面式子的最小化。$$\ell(\theta) = \sum_{i=1}^m(-y_i\theta^Tx_i + \ln (1+e^{\theta^Tx_i}))$$ 从另外一个角度，当我们令似然项$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$ 可以同样的列出相应的最大似然函数，$$\begin{split}L(\theta) =&amp; \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\=&amp;\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\end{split}$$ 同理的得其对数似然如下， $$\begin{split}\ell(\theta) &amp;= \log L(\theta) \\&amp;= \sum_{i=1}^m [y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) ]\\&amp;=\sum_{i=1}^m [y^{(i)} \log \frac{h_\theta(x^{(i)})}{1-h_\theta(x^{(i)})} + \log(1-h_\theta(x^{(i)}))]\\&amp;=\sum_{i=1}^m[y_i\theta^Tx_i - \log(1+e^{\theta^Tx_i})]\end{split}$$ 即求上式的最大值。 因此，我们可以将代价函数用如下的式子来表示，（目的是要通过代价函数的最小值来估计出相应的参数）$$Cost(h_\theta,y) = -y\log(h_\theta(x)) - (1-y) \log(1-h_\theta(x))$$因此得到代价函数$J(\theta)$$${J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]}$$后面的步骤就跟线性回归很相似了，可以直接用梯度下降法。$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac {1}{m} \sum _{i=1} ^m{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ 12345678%calculation Jz = X*theta;t = -y' * log(sigmoid(z)) - (1 - y') * log (1 - sigmoid(z));J = t / m; %calculation gradgrad_t = (sigmoid(z) - y);grad =( X' * grad_t )/ m; Multi-nominal logistic regression modelLogistic Regression只能用于二分类系统，对于一个多分类系统（K分类），我们假设离散变量$Y$的取值集合是{${1,2,3,\dots,K}$}，那么多项逻辑斯蒂回归模型是$$\begin {split}p(Y = k &amp;| x) = \frac{e^{\theta_{k}^{T}x}}{1+\sum_{k=1}^{K-1}e^{\theta_{k}^{T}x}}\\&amp;k = 1 ,2,3\dots,k-1\end{split}$$ $$p(Y = K| x) = \frac{1}{1+\sum_{k=1}^{K-1}e^{\theta_{k}^{T}x}}$$ 参考文献 《机器学习》周志华老师著 《统计学习方法》李航老师著 Couresera Machine Learning Andrew-Ng understanding-logistic-regression-using-odds Machine-Learning-Andrew-Ng-My-Notes logistic-regression 终于搞清楚什么是逻辑回归-对数几率回归]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Linear Regression]]></title>
    <url>%2F2017%2F05%2F03%2FML-Linear-Regression%2F</url>
    <content type="text"><![CDATA[Linear Regression 线性回归线性回归，给定一个样本集合 $D=(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)$ 这里的$x_i,y_i$都可以是高维向量，可以找到一个线性模拟$f(x_i)=wx_i+b$，只要确定了$w$跟$b$，这个线性模型就确定了，如何评估样本$y$与你的$f(x)$之间的差别，最常用的方法是最小二乘法。 也就是说，我们用一条直线去模拟当前的样本，同时预测未来的数据，即我们给定某一个$x$值，根据模型可以返回给我们一个$y$值，这就是线性回归。 为了表示方便，我们使用如下的形式表示假设函数，为了方便 ${h_{\theta}(x)}$ 也可以记作 $h(x)$。 $$h_\theta(x) = \theta_0 + \theta_1x$$现在的问题是我们如何选择$\theta_0$跟$\theta_1$ ，使得对于训练样本$(x,y)$，$h(x)$最『接近』$y$。$h(x)$与$y$越是接近，说明假设函数越是准确。这里我们选择均方误差作为衡量标准，即每个样本的估计值与实际值之间的差的平方的均值最小。 用公式表达为$${\mathop{min}_{\theta_0,\theta_1}} \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$$\frac{1}{2m}$是为了后续求导的计算方便，不影响最小化均方误差。 下面引入代价函数（cost function）。 Linear Regression cost function$$J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$ 123X = [ones(m,1),data(:,1)]; %Add a column of ones to xZ = (X*theta - y).^2; J = sum(Z(:,1)) / (2*m); 也就是我们的优化目标，使得代价函数$J(\theta_0,\theta_1)$最小，即${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) }$ 对应于不同的$\theta_0$ $\theta_1$ ，函数$h_\theta(x) = \theta_0 + \theta_1x$表示不同的直线。如下图，是我们最理想的情况。 我们需要不断的尝试不同的$\theta_0$$\theta_1$直到找到一个最佳的$h_\theta(x)$。是否有特定的算法，来帮助我们自动的找到最佳的$h_\theta(x)$呢？下面我们介绍梯度下降法。 梯度下降（Gradient descent）梯度下降法是一种优化方法，它可以帮助我们快速的找到一个函数的局部极小值点，也就是$\mathop{min}J(\theta_0,\theta_1)$。它的基本思想是：我们先随机一个初始的$\theta_0$$\theta_1$，通过不断的改变它们的值，使得$J(\theta)$变小，最终找到$J(\theta)$的最小值点。 为了更好的理解梯度下降法，我们同时设置的$\theta_0$$\theta_1$的值，再绘出$J(\theta_0,\theta_1)$的图形，因为有两个变量，因此$J(\theta_0,\theta_1)$的图形为一个曲面。如下所示，图形的最低点即为我们想要求得的$\mathop{min}J(\theta_0,\theta_1)$。 3D的图形不方便研究Gradient descent因此我们使用二维的等高线，同一等高线上的点对应的$J(\theta_0,\theta_1)$的值相同，如下图所示，越靠近二维等高线的中心，表示$J(\theta_0,\theta_1)$的值越小。 Have some function ${J(\theta_0,\theta_1) }$ want ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ Outline: Start with some $\theta_0,\theta_1$ keep changing $\theta_0,\theta_1$ to reduce ${J(\theta_0,\theta_1) }$ until we hopefully end up at ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ 下面看看算法的具体过程，如下所示，其中$\alpha$叫做学习率（learn rate）用来控制梯度下降的幅度，$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$叫做梯度（代价函数对每个$\theta$的偏导）。这里要注意的是每次必须同时的改变$\theta_0$和$\theta_1$的值。 Gradient descent algorithmrepeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$$(simultaneously update $\theta_j$ for $j = 0$ and $j = 1$) } 当$j=0$时， $\frac{\partial}{\partial{\theta_0}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ $\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ 当$j=1$时， $\frac{\partial}{\partial{\theta_1}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ $\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ 12Z = X' * (X * theta - y) * alpha / m;theta = theta - Z; 学习率$\alpha$会影响梯度下降的幅度，如果$\alpha$太小，$\theta$的每次变化幅度会很小，梯度下降的速度就会很慢，如果$\alpha$过大，则$\theta$每次的变化幅度就会很大，有可能使得$J(\theta)$越过最低点，永远无法收敛到最小值。随着$J(\theta)$越来越接近最低点，对应的梯度值$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$也会越来越小，每次下降的程度也会越来越慢，因此我们并不需要刻意的去减少$\alpha$的值。 事实上，由于线性回归的代价函数总是一个凸函数（Convex Function）这样的函数没有局部最优解，只有一个全局最优解，因此我们在使用梯度下降的时候，总会找到一个全局的最优解。 Linear Regression with Mulitiple Variables在实际问题中，输入的数据会有很多的特征值，而不仅仅只有一个。这里我们约定，用$n$来表示特征的数量，$m$表示训练样本的数量。$x^{(i)}$表示第$i$个训练样本，$x^{(i)}_j$表示第$i$个训练样本的第$j$个特征值。 单元线性回归中的假设函数为$$h_\theta(x) = \theta_0 + \theta_1x$$同理类比得，在多元线性回归中的假设函数为 $$h_\theta(x) = \theta_0 + \theta_1 x_1+ \theta_2 x_2+ \cdots+\theta_n x_n$$ $$h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 +\cdots+ \theta_n x_n = \theta^T x$$ cost function多元线性回归的代价函数跟一元线性回归的代价函数相同。只是$x$由Scale Value变为了Vector Value。$$J(\theta) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$ 梯度下降（Gradient descent）repeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta) }$$(simultaneously update $\theta_j$ for $j = 0 ,\cdots ,n$ ) }$$\frac{\partial}{\partial{\theta_j}} {J(\theta)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ $$\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_0$$ $$\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_1$$ $$\theta_2 := \theta_2 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_2$$ ​ ($x^{(i)}_0$ =1) 特征放缩对于多元线性回归，如果每个特征值的相差范围很大，梯度下降的速度会很慢，这时候就需要对特征值数据做缩放处理（Feature Scaling），从而将所有特征的数据数量级都放缩在一个很小的范围内，以加快梯度下降的速度。 常用的特征处理的方法就是均值归一化（Mean Normalization）$$x_i = \frac{x_i - \mu_i}{\max-\min}$$或者$$x_i = \frac{x_i - \mu_i}{\sigma_i}$$ 正规方程当我们使用梯度下降法去求未知参数$\theta$的最优值时，需要通过很多次的迭代才能得到全局最优解，有的时候梯度下降的速度会很慢。有没有其他更好的方法呢？假设代价函数$J(\theta) = a\theta^2+b\theta+c$，$\theta$是一个实数，求$\theta$的最优解，只需要令它的导数为零。 事实上的$\theta$是一个$n+1$维向量，需要我们对每个$\theta_i$求偏导，令偏导为零，就可以求出每个$\theta_i$的值。 首先, 在数据集前加上一列$x_0$, 值都为1；然后将所有的变量都放入矩阵$X$中(包括加上的$x_0$)；再将输出值放入向量$y$中，最后通过公式$\theta = (X^TX)^{-1}X^Ty$, 就可以算出$\theta$的值。这个公式就叫做正规方程，正规方程不需要进行特征放缩。 对于正规方程中$X^TX$不可逆的情况，可能是我们使用了冗余的特征，特征的数量超过了样本的数量，我们可以通过删掉一些不必要的特征或者使用正则化来解决。 1theta = pinv(X'*x)*x'*y; Gradient Descent VS Normal Equation Gradient Descent Normal Equation Need to choose $\alpha$. No need to choose $\alpha$. Need many iterations. No need to iterate. Works well even when n is large. need to compute $(X^TX)^{-1}$,slow if n is very large. 参考文献 《机器学习》周志华老师著 《统计学习方法》李航老师著 Couresera Machine Learning Andrew-Ng Machine-Learning-Andrew-Ng-My-Notes]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Machine Learning 系列 目录]]></title>
    <url>%2F2017%2F05%2F01%2FML-directory%2F</url>
    <content type="text"><![CDATA[小记 Machine Learning 系列 目录 小记 Linear Regression 小记 Logistic Regression 小记 Neural Network 小记 Gradient Descent 小记 SVM 一点小吐槽，这个系列还是越写越变成了读书笔记？本来是希望写写自己的心得，到后面发现自己写的东西都是对书上的东西的理解，既然是这样的话，就争取把笔记做好。 自己在读的资料主要是下面的一些 《机器学习》周志华老师著 《统计学习方法》李航老师著 CS231n Convolution Neual Networks for Visual Recognition UFLDL Tutorial Couresera Machine Learning Andrew-Ng 之后的计划会继续读一下 《Pattern Recognition and Machine Learning》M. Jordan J. Kleinberg B. Scho ̈lkopf 著 《Deep Learning》Ian Goodfellow Yoshua Bengio Aaron Courville 著 两本大作，可以回头来对自己所写有一个更好的补充。：）]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
        <tag>directory</tag>
      </tags>
  </entry>
</search>