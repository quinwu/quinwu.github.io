<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小记 overfitting 与 underfitting]]></title>
    <url>%2F2017%2F05%2F06%2FML-overfitting-underfitting%2F</url>
    <content type="text"></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Linear Regression 与 Logistic Regression]]></title>
    <url>%2F2017%2F05%2F03%2FML-LinearReg-LogisticReg%2F</url>
    <content type="text"><![CDATA[Linear Regression 线性回归 Logistic Regression 对数几率回归 Linear Regression 线性回归回归的本身是一种基于数据的建模，一般而言，线性回归（Linear Regression）相对比较简单，而对数几率回归（Logistic Regression）的内容要丰富很多。 线性回归很简单，给定一个样本集合 $D=(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)$ 这里的$x_i,y_i$都可以是高维向量，可以找到一个线性模拟$f(x_i)=wx_i+b$，只要确定了$w$跟$b$，这个线性模型就确定了，如何评估样本$y$与你的$f(x)$之间的差别，最常用的方法是最小二乘法。 也就是说，我们用一条直线去模拟当前的样本，同时预测未来的数据，即我们给定某一个$x$值，根据模型可以返回给我们一个$y$值，这就是线性回归。 为了表示方便，我们使用如下的形式表示假设函数，为了方便 ${h_{\theta}(x)}$ 也可以记作 $h(x)$。 $$h_\theta(x) = \theta_0 + \theta_1x$$现在的问题是我们如何选择$\theta_0$跟$\theta_1$ ，使得对于训练样本$(x,y)$，$h(x)$最『接近』$y$。$h(x)$与$y$越是接近，说明假设函数越是准确。这里我们选择均方误差作为衡量标准，即每个样本的估计值与实际值之间的差的平方的均值最小。 用公式表达为$${\mathop{min}_{\theta_0,\theta_1}} \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$$\frac{1}{2m}$是为了后续求导的计算方便，不影响最小化均方误差。 下面引入代价函数（cost function）。 Linear Regression cost function$$J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$ 123X = [ones(m,1),data(:,1)]; %Add a column of ones to xZ = (X*theta - y).^2; J = sum(Z(:,1)) / (2*m); 也就是我们的优化目标，使得代价函数$J(\theta_0,\theta_1)$最小，即${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) }$ 对应于不同的$\theta_0$ $\theta_1$ ，函数$h_\theta(x) = \theta_0 + \theta_1x$表示不同的直线。如下图，是我们最理想的情况。 我们需要不断的尝试不同的$\theta_0$$\theta_1$直到找到一个最佳的$h_\theta(x)$。是否有特定的算法，来帮助我们自动的找到最佳的$h_\theta(x)$呢？下面我们介绍梯度下降法。 梯度下降（Gradient descent）梯度下降法是一种优化方法，它可以帮助我们快速的找到一个函数的局部极小值点，也就是$\mathop{min}J(\theta_0,\theta_1)$。它的基本思想是：我们先随机一个初始的$\theta_0$$\theta_1$，通过不断的改变它们的值，使得$J(\theta)$变小，最终找到$J(\theta)$的最小值点。 为了更好的理解梯度下降法，我们同时设置的$\theta_0$$\theta_1$的值，再绘出$J(\theta_0,\theta_1)$的图形，因为有两个变量，因此$J(\theta_0,\theta_1)$的图形为一个曲面。如下所示，图形的最低点即为我们想要求得的$\mathop{min}J(\theta_0,\theta_1)$。 3D的图形不方便研究Gradient descent因此我们使用二维的等高线，同一等高线上的点对应的$J(\theta_0,\theta_1)$的值相同，如下图所示，越靠近二维等高线的中心，表示$J(\theta_0,\theta_1)$的值越小。 Have some function ${J(\theta_0,\theta_1) }$ want ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ Outline: Start with some $\theta_0,\theta_1$ keep changing $\theta_0,\theta_1$ to reduce ${J(\theta_0,\theta_1) }$ until we hopefully end up at ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ 下面看看算法的具体过程，如下所示，其中$\alpha$叫做学习率（learn rate）用来控制梯度下降的幅度，$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$叫做梯度（代价函数对每个$\theta$的偏导）。这里要注意的是每次必须同时的改变$\theta_0$和$\theta_1$的值。 Gradient descent algorithmrepeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$$(simultaneously update $\theta_j$ for $j = 0$ and $j = 1$) } 当$j=0$时， $\frac{\partial}{\partial{\theta_0}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ $\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ 当$j=1$时， $\frac{\partial}{\partial{\theta_1}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ $\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ 12Z = X' * (X * theta - y) * alpha / m;theta = theta - Z; 学习率$\alpha$会影响梯度下降的幅度，如果$\alpha$太小，$\theta$的每次变化幅度会很小，梯度下降的速度就会很慢，如果$\alpha$过大，则$\theta$每次的变化幅度就会很大，有可能使得$J(\theta)$越过最低点，永远无法收敛到最小值。随着$J(\theta)$越来越接近最低点，对应的梯度值$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$也会越来越小，每次下降的程度也会越来越慢，因此我们并不需要刻意的去减少$\alpha$的值。 事实上，由于线性回归的代价函数总是一个凸函数（Convex Function）这样的函数没有局部最优解，只有一个全局最优解，因此我们在使用梯度下降的时候，总会找到一个全局的最优解。 Linear Regression with Mulitiple Variables在实际问题中，输入的数据会有很多的特征值，而不仅仅只有一个。这里我们约定，用$n$来表示特征的数量，$m$表示训练样本的数量。$x^{(i)}$表示第$i$个训练样本，$x^{(i)}_j$表示第$i$个训练样本的第$j$个特征值。 单元线性回归中的假设函数为$$h_\theta(x) = \theta_0 + \theta_1x$$同理类比得，在多元线性回归中的假设函数为 $$h_\theta(x) = \theta_0 + \theta_1 x_1+ \theta_2 x_2+ \cdots+\theta_n x_n$$ $$h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 +\cdots+ \theta_n x_n = \theta^T x$$ 梯度下降（Gradient descent）repeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$$(simultaneously update $\theta_j$ for $j = 0 ,\cdots ,n$ ) }$$\frac{\partial}{\partial{\theta_j}} {J(\theta)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ $$\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_0$$ $$\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_1$$ $$\theta_2 := \theta_2 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_2$$ ​ ($x^{(i)}_0$ =1) 特征放缩todo 正规方程todo Logistic Regression 对数几率回归接下来介绍下广义线性回归，也很简单，我们不再只用线性函数来模拟数据，而是在外层添加了一个单调可微函数$g(z)$，即$f(x_i) = g(wx_i+b) $ ，如果 $ g=ln(x) $，则这个广义线性回归就变成了对数线性回归，其本质就是给原来线性变换加上了一个非线性变换，使得模拟的函数有非线性的属性。但本质上的参数还是线性的，主体是内部线性的调参。 如果我们觉得模型应该是指数变化的时候，我们可以简单粗暴的把线性模型映射到指数变化上，如上图中的红线映射到黑色的指数线，这就是广义线性模型的思想 对数几率回归（Logistic Regression）不是解决回归问题的，而是解决分类问题的。目的是要构造出一个分类器（Classifier）。对数几率回归（Logistic Regression）的关键并不在于回归，而在于对数几率函数。 对一个简单的二分类问题，实际上是样本点或者预测点到一个值域为 的函数，函数值表示这个点分在正类（postive）或者反类（negtive）的概率，如果非常可能是正类（postive），那么其概率值就逼近与1，如果非常可能是反类（negtive）其概率值就逼近与0。 构造一个sigmoid函数 $y=\frac{1}{1+ \mathrm{e}^{-z} } $。 1y = 1 ./ ( 1 + exp(-z) ); 在对数几率回归（Logistic Regression），假设函数为$h_\theta (x) = g (\theta^T x)$，其中$g(z)=\frac{1}{1+ \mathrm{e}^{-z} } $就是我们上面构造的sigmoid函数。即，$$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$$我们可以将对率函数的输出理解为，当输入为$x$的时候，$y=1$的概率，可以用$h_\theta(x) = P(y = 1 | x;\theta)$ 表达。对于sigmoid函数，当$z &gt; 0$时 ，$g(z) \geq 0.5$ 既预测 $y = 1$，当$z &lt; 0$时，$g(z) &lt; 0.5$ 即预测$ y = 0$。 Logistic Regression cost function跟线性回归中的代价函数相比，$$J(\theta) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$线性回归之所以可以使用梯度下降法来下降到最优解是因为代价函数$J(\theta)$是一个凸函数。对于对率回归而言，假设函数$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 是一个非线性的复杂模型，代价函数就不是一个凸函数(non-convex)。这样使用梯度下降法就只能得到局部最优解而非全局最优解，所以我们需要构造一个合理的并且是凸函数的代价函数。 对于$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 可以变化为$\ln\frac{y}{1-y} = \theta^Tx$，若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例的可能性，两者的比值$\frac{y}{1-y}$成为几率（odds），反映了$x$作为正例的相对可能性，对几率取对数则得到了对数几率（log odds ，亦称logit）$\ln\frac{y}{1-y}$。$$\ln\frac{p(y=1 | x)}{p(y = 0| x)} = \theta^Tx$$显然有$$p(y = 1|x) = \frac{e^{\theta^Tx}}{1+e^{\theta^Tx}} = \frac{1}{1+e^{-\theta^Tx}}$$ $$p(y = 0|x) = \frac{1}{1+e^{\theta^Tx}}$$ 于是我们可以通过极大似然法（maximum likehood method）来估计$\theta$，得到似然函数：$$L(\theta) = \prod_{i=1}^mp(y_i|x_i;\theta)$$ 对率回归模型最大化对数似然（log-likehood）$$\ell(\theta) = \sum_{i=1}^m\ln p(y_i|x_i;\theta)$$即令每个样本属于其真实标记的概率越大越好。为了方便讨论，我们令$p_1(x;\theta) = p(y = 1|x;\theta) $，$p_0(x;\theta) = p( y = 0 | x;\theta)$，则上式子中的似然项可以重写为$$p(y_i|x_i;\theta) = y_ip_1(x_i;\theta) + (1 - y_i)p_0(x_i;\theta)$$带入对率回归模型中得，$$\ell(\theta) = \sum_{i=1}^m \ln [y_i p_1(x_i;\theta) + (1 - y_i) p_0(x_i;\theta)]$$ 得到了对数似然函数，通过求对数似然函数的最大值来估计模型参数。根据已知的$p0$$p1$，可得，上述对数似然函数的最大化式等价于下面式子的最小化。$$\ell(\theta) = \sum_{i=1}^m(-y_i\theta^Tx_i + \ln (1+e^{\theta^Tx_i}))$$ 从另外一个角度，当我们令似然项$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$ 可以同样的列出相应的最大似然函数，$$\begin{split}L(\theta) =&amp; \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\=&amp;\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\end{split}$$ 同理的得其对数似然如下， $$\begin{split}\ell(\theta) &amp;= \log L(\theta) \\&amp;= \sum_{i=1}^m [y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) ]\\&amp;=\sum_{i=1}^m [y^{(i)} \log \frac{h_\theta(x^{(i)})}{1-h_\theta(x^{(i)})} + \log(1-h_\theta(x^{(i)}))]\\&amp;=\sum_{i=1}^m[y_i\theta^Tx_i - \log(1+e^{\theta^Tx_i})]\end{split}$$ 即求上式的最大值。 因此，我们可以将代价函数用如下的式子来表示，（目的是要通过代价函数的最小值来估计出相应的参数）$$Cost(h_\theta,y) = -y\log(h_\theta(x)) - (1-y) \log(1-h_\theta(x))$$因此得到代价函数$J(\theta)$$${J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]}$$后面的步骤就跟线性回归很相似了，可以直接用梯度下降法。$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac {1}{m} \sum _{i=1} ^m{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ 12345678%calculation Jz = X*theta;t = -y' * log(sigmoid(z)) - (1 - y') * log (1 - sigmoid(z));J = t / m; %calculation gradgrad_t = (sigmoid(z) - y);grad =( X' * grad_t )/ m; 参考文献 机器学习 周志华著 统计学习方法 李航著 Couresera Machine Learning Andrew-Ng understanding-logistic-regression-using-odds Machine-Learning-Andrew-Ng-My-Notes logistic-regression 终于搞清楚什么是逻辑回归-对数几率回归]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Machine Learning 系列 目录]]></title>
    <url>%2F2017%2F05%2F01%2FML-directory%2F</url>
    <content type="text"></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
        <tag>directory</tag>
      </tags>
  </entry>
</search>