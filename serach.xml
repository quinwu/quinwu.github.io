<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[小记 Neural Network]]></title>
    <url>%2F2017%2F05%2F16%2FML-Neural-Network%2F</url>
    <content type="text"><![CDATA[logistic regression cost function$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)}) ) +(1-y^{(i)})\log(1-h_\theta(x^{(i)}))$$ neural network$$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k) \Bigg]$$ logistic regression cost function regularization$$J(\theta) = -\frac{1}{m} \sum_{i=1}^m y^{(i)}\log(h_\theta(x^{(i)}) ) +(1-y^{(i)})\log(1-h_\theta(x^{(i)})) + \frac{\lambda}{2m} \sum_{j=1}^n \theta_j^2$$ neural network regularization$$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\Bigg] + \frac{\lambda}{2m} \sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2$$ 代价函数与反向传播 Back propagation一些标记: L 表示神经网络的总层数 $S_4$表示第四层神经网络，不包括偏差单元bias unit k表示第几个输出单元 Feed forward computation $h_\theta(x^{(i)})$ 1234567891011% computation h(x)% input layerxa1 = [ones(m,1) X];% hidden layerZ2 = a1*Theta1';a2 = sigmoid(Z2);a2 = [ones(size(a2,1),1) a2];% output layerZ3 = a2*Theta2';a3 = sigmoid(Z3);h = a3; $$J(\Theta) = -\frac{1}{m}\Bigg[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)} \log(h_\Theta(x^{(i)}))_k + (1- y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k) \Bigg]$$ 1234567891011121314151617181920%case 1J = 0;Y = zeros(m,num_labels);for i = 1 : m Y(i,y(i)) = 1;endJ = -1/m * (Y * log(h)' + (1 - Y) * log(1 - h)');J = trace(J);%case 2J = 0;Y = zeros(m,num_labels);for i = 1 : m Y(i,y(i)) = 1;endfor i = 1 : m J = J + -1*m *(Y(i,:) * log(h(i,:))' + (1 - Y(i,:)* log(1 - h(i,:))');end back propagation我们知道代价函数cost function后，下一步就是按照梯度下降法来计算$\theta$求解cost function的最优解。使用梯度下降法首先要求出梯度，即偏导项$\frac{\partial}{\partial \Theta^{(l)} _{ij}} J(\Theta)$，计算偏导项的过程我们称为back propagation。 根据上面的feed forward computation 我们已经计算得到了 $a^{(1)}$ ，$a^{(2)}$， $a^{(3)}$ ，$Z^{(2)}$，$Z^{(3)}$。 在back propagation中，我们定义每层的误差$$\delta^{(l)} = \frac {\partial}{ \partial z^{(l)}} J(\Theta)$$$\delta^{(l)}_j$ 表示第$l$层第$j$个节点的误差。为了求出偏导项$\frac{\partial}{\partial \Theta^{(l)} _{ij}} J(\Theta)$，我们首先要求出每一层的$\delta$（不包括第一层，第一层是输入层，不存在误差），对于输出层第三层$$\begin{align}\delta_i^{(4)} &amp; = \frac{\partial}{\partial z_i^{(4)}}J(\Theta) \\ \\&amp; = \frac{\partial J(\Theta)}{\partial a_i^{(4)}}\frac{\partial a_i^{(4)}}{\partial z_i^{(4)}} \\ \\&amp; = -\frac{\partial}{\partial a_i^{(4)}}\sum_{k=1}^K\left[y_kloga_k^{(4)}+(1-y_k)log(1-a_k^{(4)})\right]g’(z_i^{(4)}) \\ \\&amp; = -\frac{\partial}{\partial a_i^{(4)}}\left[y_iloga_i^{(4)}+(1-y_i)log(1-a_i^{(4)})\right]g(z_i^{(4)})(1-g(z_i^{(4)})) \\\\ &amp; = \left(\frac{1-y_i}{1-a_i^{(4)}}-\frac{y_i}{a_i^{(4)}}\right)a_i^{(4)}(1-a_i^{(4)}) \\\\ &amp; = (1-y_i)a_i^{(4)} - y_i(1-a_i^{(4)}) \\\\ &amp; = a_i^{(4)} - y_i \\\end{align}$$ $$\begin{split}\delta_i^{(l)} &amp; = \frac{\partial}{\partial z_i^{(l)}}J(\Theta) \\\\ &amp; = \sum_{j=1}^{S_j}\frac{\partial J(\Theta)}{\partial z_j^{(l+1)}}\cdot\frac{\partial z_j^{(l+1)}}{\partial a_i^{(l)}}\cdot\frac{\partial a_i^{(l)}}{\partial z_i^{(l)}} \\ \\ &amp; = \sum_{j=1}^{S_j}\delta_j^{(l+1)}\cdot\Theta_{ij}^{(l)}\cdot g’(z_i^{(l)}) \\\\ &amp; = g’(z_i^{(l)})\sum_{j=1}^{S_j}\delta_j^{(l+1)}\cdot\Theta_{ij}^{(l)}\end{split}$$ $$\begin{split}\frac{\partial g(z)}{\partial z} &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2\frac{\partial{}}{\partial{z}} \left(1 + e^{-z} \right) \\\\ &amp; = -\left( \frac{1}{1 + e^{-z}} \right)^2e^{-z}\left(-1\right) \\\\ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{1}{1 + e^{-z}} \right)\left(e^{-z}\right) \\\\ &amp; = \left( \frac{1}{1 + e^{-z}} \right) \left( \frac{e^{-z}}{1 + e^{-z}} \right) \\\\ &amp; = \left( \frac{1}{1+e^{-z}}\right)\left( \frac{1+e^{-z}}{1+e^{-z}}-\frac{1}{1+e^{-z}}\right) \\\\ &amp; = g(z) \left( 1 - g(z)\right) \\\\ \end{split}$$ $\Theta^{(l)}_{i,j}$ 第$l$层到第$l+1$层的权值矩阵的第$i$行第$j$列的分量 $Z^{(j)}_i$ 第$j$层第$i$个神经元的输入值 $a^{(j)}_i$第$j$层第$i$个神经元的输出值 $a^{(j)} = g(Z^{(j)})$ hidden layer to output layer$$h_\Theta(x) = a^{(L)} = g(z^{(L)})$$ $$z^{(l)} = \Theta^{(l-1)}a^{(l-1)}$$ $$\frac{\partial}{\partial \Theta^{(L-1)}_{i,j}}J(\Theta) = \frac {\partial J(\Theta)}{\partial h_\theta(x)_i} \frac{\partial h_\theta(x)_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} = \frac {\partial J(\Theta)}{\partial a^{(L)}_i}\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}}$$ $$cost(\Theta) =- y^{(i)}\log(h_\Theta(x^{(i)}) ) -(1-y^{(i)})\log(1-h_\Theta(x^{(i)}))$$ $$\frac{\partial J(\Theta)}{\partial a^{(L)}_i} =\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i}$$ $$\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} = \frac{\partial g(z^{(L)}_i)}{\partial z^{(L)}_i} = g(z^{(L)}_i)(1- g(z^{(L)}_i))=a^{(L)}_i(1- a^{(L)}_i)$$ $$\frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} = a^{(L-1)}_j$$ 综上$$\begin{split}\\ \frac{\partial}{\partial \Theta^{(L-1)}_{i,j}}J(\Theta) &amp;= \frac {\partial J(\Theta)}{\partial a^{(L)}_i}\frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \frac{\partial z^{(L)}_i}{\partial \Theta^{(L-1)}_{i,j}} \\\\ &amp;=\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i} a^{(L)}_i(1- a^{(L)}_i) a^{(L-1)}_j \\\\ &amp;= (a^{(L)}_i - y_i) a_j^{(L-1)}\end{split}$$ hidden layer / input layer to hidden layer因为$a^{(1)} = x$，所以可以将 input layer 与 hidden layer同样对待$$\frac{\partial}{\partial \Theta^{(l-1)}_{i,j}}J(\Theta) = \frac {\partial J(\Theta)}{\partial a^{(l)}_i} \frac{\partial a^{(l)}_i}{\partial z^{(l)}_i}\frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \ (l = 2, …, L-1)$$ $$\frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} =\frac{\partial g(z^{(l)}_i)}{\partial z^{(l)}_i} =g(z^{(l)}_i)(1- g(z^{(l)}_i))=a^{(l)}_i(1- a^{(l)}_i)$$ $$\frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} = a^{(l-1)}_j$$ 第一部分的偏导比较麻烦，要使用chain rule。$$\dfrac{\partial J(\Theta)}{\partial a_i^{(l)}}= \sum_{k=1}^{s_{l+1}} \Bigg[\dfrac{\partial J(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]$$ $$\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} = a^{(l+1)}_k (1 - a^{(l+1)}_k)$$ $$\frac{\partial z^{(l+1)}_k}{\partial a^{(l)}_i} = \Theta^{(l)}_{k,i}$$ 求得递推式为：$$\begin{split}\\ \frac{\partial J(\Theta)}{\partial a^{(l)}_i} &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\dfrac{\partial J(\Theta)}{\partial a_k^{(l+1)}} \dfrac{\partial a_k^{(l+1)}}{\partial z_k^{(l+1)}} \dfrac{\partial z_k^{(l+1)}}{\partial a_i^{(l)}}\Bigg]\\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\frac{\partial J(\Theta)}{\partial a^{(k+1)}_k}\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} \Theta^{(l)}_{k,i} \Bigg] \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[ \frac{\partial J(\Theta)}{\partial a^{(l+1)}_k}a^{(l+1)}_k (1 - a^{(l+1)}_k) \Theta^{(l)}_{k,i} \Bigg]\end{split}$$定义第$l$层第$i$个节点的误差为：$$\begin{split}\delta^{(l)}_i &amp;= \frac{\partial J(\Theta)}{\partial a^{(l)}_i} \frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} \\\\ &amp;=\frac{\partial J(\Theta)}{\partial a^{(l)}_i} a^{(l)}_i (1 - a^{(l)}_i) \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\frac{\partial J(\Theta)}{\partial a^{(k+1)}_k}\frac{\partial a^{(l+1)}_k}{\partial z^{(l+1)}_k} \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i) \\\\ &amp;= \sum_{k=1}^{s_{l+1}} \Bigg[\delta^{(l+1)}_k \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i) \\end{split}$$ $$\begin{split}\delta^{(L)}_i &amp;= \frac{\partial J(\Theta)}{\partial z^{(L)}_i} \\\\ &amp;= \frac {\partial J(\Theta)}{\partial a^{(L)}_i} \frac{\partial a^{(L)}_i}{\partial z^{(L)}_i} \\\\ &amp;=\frac{a^{(L)}_i -y_i}{(1-a^{(L)}_i)a^{(L)}_i} a^{(L)}_i(1- a^{(L)}_i) \\\\ &amp;= a^{(L)}_i - y_i\end{split}$$ 最终代价函数的偏导数为$$\begin{split}\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta) &amp;= \frac {\partial J(\Theta)}{\partial a^{(l)}_i}\frac{\partial a^{(l)}_i}{\partial z^{(l)}_i} \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\&amp;= \frac {\partial J(\Theta)}{\partial z^{(l)}_i} \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\ &amp;= \delta^{(l)}_i \frac{\partial z^{(l)}_i}{\partial \Theta^{(l-1)}_{i,j}} \\\\ &amp;= \delta^{(l)}_i a^{(l-1)}_j\end{split}$$总结 输出层的误差 $\delta^{(L)}_i$$$\delta^{(L)}_i = a^{(L)}_i - y_i$$ 隐层误差 $\delta^{(l)}_i$$$\delta^{(l)}_i == \sum_{k=1}^{s_{l+1}} \Bigg[\delta^{(l+1)}_k \Theta^{(l)}_{k,i} \Bigg] a^{(l)}_i (1 - a^{(l)}_i)$$ 代价函数偏导项 $\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta)$$$\frac {\partial}{\partial \Theta^{(l-1)}_{i,j}} J(\Theta) = \delta^{(l)}_i a^{(l-1)}_j$$​ Chain Rule$y = g(x) $ $z = h(y)$ $\Delta x \rightarrow \Delta y \rightarrow \Delta z$ $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$ $x = g(s)$ $y = h(s)$ $z = k(x,y)$ $\frac{dz}{ds} = \frac{\partial z}{\partial x} \frac{dx}{ds} + \frac{\partial z }{\partial y} \frac{dy}{ds}$ ​ 123456delta_3 = h - Y;delta_2 = delta_3 * Theta2 .* a2 .*(1 - a2);delta_2 = delta_2(:,2:end);Theta1_grad = delta_2' * a1 / m;Theta2_grad = delta_3' * a2 / m Chain Rule$y = g(x) $ $z = h(y)$ $\Delta x \rightarrow \Delta y \rightarrow \Delta z$ $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$ $x = g(s)$ $y = h(s)$ $z = k(x,y)$ $\frac{dz}{ds} = \frac{\partial z}{\partial x} \frac{dx}{ds} + \frac{\partial z }{\partial y} \frac{dy}{ds}$ ​]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 overfitting 与 underfitting]]></title>
    <url>%2F2017%2F05%2F06%2FML-overfitting-underfitting%2F</url>
    <content type="text"></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Linear Regression 与 Logistic Regression]]></title>
    <url>%2F2017%2F05%2F03%2FML-LinearReg-LogisticReg%2F</url>
    <content type="text"><![CDATA[Linear Regression 线性回归 Logistic Regression 对数几率回归 Linear Regression 线性回归回归的本身是一种基于数据的建模，一般而言，线性回归（Linear Regression）相对比较简单，而对数几率回归（Logistic Regression）的内容要丰富很多。 线性回归很简单，给定一个样本集合 $D=(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)$ 这里的$x_i,y_i$都可以是高维向量，可以找到一个线性模拟$f(x_i)=wx_i+b$，只要确定了$w$跟$b$，这个线性模型就确定了，如何评估样本$y$与你的$f(x)$之间的差别，最常用的方法是最小二乘法。 也就是说，我们用一条直线去模拟当前的样本，同时预测未来的数据，即我们给定某一个$x$值，根据模型可以返回给我们一个$y$值，这就是线性回归。 为了表示方便，我们使用如下的形式表示假设函数，为了方便 ${h_{\theta}(x)}$ 也可以记作 $h(x)$。 $$h_\theta(x) = \theta_0 + \theta_1x$$现在的问题是我们如何选择$\theta_0$跟$\theta_1$ ，使得对于训练样本$(x,y)$，$h(x)$最『接近』$y$。$h(x)$与$y$越是接近，说明假设函数越是准确。这里我们选择均方误差作为衡量标准，即每个样本的估计值与实际值之间的差的平方的均值最小。 用公式表达为$${\mathop{min}_{\theta_0,\theta_1}} \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$$\frac{1}{2m}$是为了后续求导的计算方便，不影响最小化均方误差。 下面引入代价函数（cost function）。 Linear Regression cost function$$J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$ 123X = [ones(m,1),data(:,1)]; %Add a column of ones to xZ = (X*theta - y).^2; J = sum(Z(:,1)) / (2*m); 也就是我们的优化目标，使得代价函数$J(\theta_0,\theta_1)$最小，即${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) }$ 对应于不同的$\theta_0$ $\theta_1$ ，函数$h_\theta(x) = \theta_0 + \theta_1x$表示不同的直线。如下图，是我们最理想的情况。 我们需要不断的尝试不同的$\theta_0$$\theta_1$直到找到一个最佳的$h_\theta(x)$。是否有特定的算法，来帮助我们自动的找到最佳的$h_\theta(x)$呢？下面我们介绍梯度下降法。 梯度下降（Gradient descent）梯度下降法是一种优化方法，它可以帮助我们快速的找到一个函数的局部极小值点，也就是$\mathop{min}J(\theta_0,\theta_1)$。它的基本思想是：我们先随机一个初始的$\theta_0$$\theta_1$，通过不断的改变它们的值，使得$J(\theta)$变小，最终找到$J(\theta)$的最小值点。 为了更好的理解梯度下降法，我们同时设置的$\theta_0$$\theta_1$的值，再绘出$J(\theta_0,\theta_1)$的图形，因为有两个变量，因此$J(\theta_0,\theta_1)$的图形为一个曲面。如下所示，图形的最低点即为我们想要求得的$\mathop{min}J(\theta_0,\theta_1)$。 3D的图形不方便研究Gradient descent因此我们使用二维的等高线，同一等高线上的点对应的$J(\theta_0,\theta_1)$的值相同，如下图所示，越靠近二维等高线的中心，表示$J(\theta_0,\theta_1)$的值越小。 Have some function ${J(\theta_0,\theta_1) }$ want ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ Outline: Start with some $\theta_0,\theta_1$ keep changing $\theta_0,\theta_1$ to reduce ${J(\theta_0,\theta_1) }$ until we hopefully end up at ${\mathop{min}_{\theta_0,\theta_1}} {J(\theta_0,\theta_1) } $ 下面看看算法的具体过程，如下所示，其中$\alpha$叫做学习率（learn rate）用来控制梯度下降的幅度，$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$叫做梯度（代价函数对每个$\theta$的偏导）。这里要注意的是每次必须同时的改变$\theta_0$和$\theta_1$的值。 Gradient descent algorithmrepeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$$(simultaneously update $\theta_j$ for $j = 0$ and $j = 1$) } 当$j=0$时， $\frac{\partial}{\partial{\theta_0}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ $\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} $ 当$j=1$时， $\frac{\partial}{\partial{\theta_1}} {J(\theta_0,\theta_1)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ $\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)} $ 12Z = X' * (X * theta - y) * alpha / m;theta = theta - Z; 学习率$\alpha$会影响梯度下降的幅度，如果$\alpha$太小，$\theta$的每次变化幅度会很小，梯度下降的速度就会很慢，如果$\alpha$过大，则$\theta$每次的变化幅度就会很大，有可能使得$J(\theta)$越过最低点，永远无法收敛到最小值。随着$J(\theta)$越来越接近最低点，对应的梯度值$ \frac{\partial}{\partial{\theta_j}} {J(\theta_0,\theta_1) }$也会越来越小，每次下降的程度也会越来越慢，因此我们并不需要刻意的去减少$\alpha$的值。 事实上，由于线性回归的代价函数总是一个凸函数（Convex Function）这样的函数没有局部最优解，只有一个全局最优解，因此我们在使用梯度下降的时候，总会找到一个全局的最优解。 Linear Regression with Mulitiple Variables在实际问题中，输入的数据会有很多的特征值，而不仅仅只有一个。这里我们约定，用$n$来表示特征的数量，$m$表示训练样本的数量。$x^{(i)}$表示第$i$个训练样本，$x^{(i)}_j$表示第$i$个训练样本的第$j$个特征值。 单元线性回归中的假设函数为$$h_\theta(x) = \theta_0 + \theta_1x$$同理类比得，在多元线性回归中的假设函数为 $$h_\theta(x) = \theta_0 + \theta_1 x_1+ \theta_2 x_2+ \cdots+\theta_n x_n$$ $$h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2 +\cdots+ \theta_n x_n = \theta^T x$$ cost function多元线性回归的代价函数跟一元线性回归的代价函数相同。只是$x$由Scale Value变为了Vector Value。$$J(\theta) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$ 梯度下降（Gradient descent）repeat until convergence { $$\theta_j := \theta_j - \alpha \frac{\partial}{\partial{\theta_j}} {J(\theta) }$$(simultaneously update $\theta_j$ for $j = 0 ,\cdots ,n$ ) }$$\frac{\partial}{\partial{\theta_j}} {J(\theta)} = \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ $$\theta_0 := \theta_0 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_0$$ $$\theta_1 := \theta_1 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_1$$ $$\theta_2 := \theta_2 - \alpha \frac {1} {m} \sum_{i = 1} ^ {m}{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_2$$ ​ ($x^{(i)}_0$ =1) 特征放缩对于多元线性回归，如果每个特征值的相差范围很大，梯度下降的速度会很慢，这时候就需要对特征值数据做缩放处理（Feature Scaling），从而将所有特征的数据数量级都放缩在一个很小的范围内，以加快梯度下降的速度。 常用的特征处理的方法就是均值归一化（Mean Normalization）$$x_i = \frac{x_i - \mu_i}{\max-\min}$$或者$$x_i = \frac{x_i - \mu_i}{\sigma_i}$$ 正规方程当我们使用梯度下降法去求未知参数$\theta$的最优值时，需要通过很多次的迭代才能得到全局最优解，有的时候梯度下降的速度会很慢。有没有其他更好的方法呢？假设代价函数$J(\theta) = a\theta^2+b\theta+c$，$\theta$是一个实数，求$\theta$的最优解，只需要令它的导数为零。 事实上的$\theta$是一个$n+1$维向量，需要我们对每个$\theta_i$求偏导，令偏导为零，就可以求出每个$\theta_i$的值。 首先, 在数据集前加上一列$x_0$, 值都为1；然后将所有的变量都放入矩阵$X$中(包括加上的$x_0$)；再将输出值放入向量$y$中，最后通过公式$\theta = (X^TX)^{-1}X^Ty$, 就可以算出$\theta$的值。这个公式就叫做正规方程，正规方程不需要进行特征放缩。 对于正规方程中$X^TX$不可逆的情况，可能是我们使用了冗余的特征，特征的数量超过了样本的数量，我们可以通过删掉一些不必要的特征或者使用正则化来解决。 MATLAB代码为 1theta = pinv(X'*x)*x'*y; Gradient Descent VS Normal Equation Gradient Descent Normal Equation Need to choose $\alpha$. No need to choose $\alpha$. Need many iterations. No need to iterate. Works well even when n is large. need to compute $(X^TX)^{-1}$,slow if n is very large. Logistic Regression 对数几率回归接下来介绍下广义线性回归，也很简单，我们不再只用线性函数来模拟数据，而是在外层添加了一个单调可微函数$g(z)$，即$f(x_i) = g(wx_i+b) $ ，如果 $ g=ln(x) $，则这个广义线性回归就变成了对数线性回归，其本质就是给原来线性变换加上了一个非线性变换，使得模拟的函数有非线性的属性。但本质上的参数还是线性的，主体是内部线性的调参。 如果我们觉得模型应该是指数变化的时候，我们可以简单粗暴的把线性模型映射到指数变化上，如上图中的红线映射到黑色的指数线，这就是广义线性模型的思想 对数几率回归（Logistic Regression）不是解决回归问题的，而是解决分类问题的。目的是要构造出一个分类器（Classifier）。对数几率回归（Logistic Regression）的关键并不在于回归，而在于对数几率函数。 对一个简单的二分类问题，实际上是样本点或者预测点到一个值域为 的函数，函数值表示这个点分在正类（postive）或者反类（negtive）的概率，如果非常可能是正类（postive），那么其概率值就逼近与1，如果非常可能是反类（negtive）其概率值就逼近与0。 构造一个sigmoid函数 $y=\frac{1}{1+ \mathrm{e}^{-z} } $。 1y = 1 ./ ( 1 + exp(-z) ); 在对数几率回归（Logistic Regression），假设函数为$h_\theta (x) = g (\theta^T x)$，其中$g(z)=\frac{1}{1+ \mathrm{e}^{-z} } $就是我们上面构造的sigmoid函数。即，$$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$$我们可以将对率函数的输出理解为，当输入为$x$的时候，$y=1$的概率，可以用$h_\theta(x) = P(y = 1 | x;\theta)$ 表达。对于sigmoid函数，当$z &gt; 0$时 ，$g(z) \geq 0.5$ 即预测 $y = 1$，当$z &lt; 0$时，$g(z) &lt; 0.5$ 即预测$ y = 0$。 Logistic Regression cost function跟线性回归中的代价函数相比，$$J(\theta) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)}) - y^{(i)})}^2$$线性回归之所以可以使用梯度下降法来下降到最优解是因为代价函数$J(\theta)$是一个凸函数。对于对率回归而言，假设函数$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 是一个非线性的复杂模型，代价函数就不是一个凸函数(non-convex)。这样使用梯度下降法就只能得到局部最优解而非全局最优解，所以我们需要构造一个合理的并且是凸函数的代价函数。 对于$h_\theta (x) = \frac {1}{1 + \mathrm{e}^{-\theta^T x}}$ 可以变化为$\ln\frac{y}{1-y} = \theta^Tx$，若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例的可能性，两者的比值$\frac{y}{1-y}$成为几率（odds），反映了$x$作为正例的相对可能性，对几率取对数则得到了对数几率（log odds ，亦称logit）$\ln\frac{y}{1-y}$。$$\ln\frac{p(y=1 | x)}{p(y = 0| x)} = \theta^Tx$$显然有$$p(y = 1|x) = \frac{e^{\theta^Tx}}{1+e^{\theta^Tx}} = \frac{1}{1+e^{-\theta^Tx}}$$ $$p(y = 0|x) = \frac{1}{1+e^{\theta^Tx}}$$ 于是我们可以通过极大似然法（maximum likehood method）来估计$\theta$，得到似然函数：$$L(\theta) = \prod_{i=1}^mp(y_i|x_i;\theta)$$ 对率回归模型最大化对数似然（log-likehood）$$\ell(\theta) = \sum_{i=1}^m\ln p(y_i|x_i;\theta)$$即令每个样本属于其真实标记的概率越大越好。为了方便讨论，我们令$p_1(x;\theta) = p(y = 1|x;\theta) $，$p_0(x;\theta) = p( y = 0 | x;\theta)$，则上式子中的似然项可以重写为$$p(y_i|x_i;\theta) = y_ip_1(x_i;\theta) + (1 - y_i)p_0(x_i;\theta)$$带入对率回归模型中得，$$\ell(\theta) = \sum_{i=1}^m \ln [y_i p_1(x_i;\theta) + (1 - y_i) p_0(x_i;\theta)]$$ 得到了对数似然函数，通过求对数似然函数的最大值来估计模型参数。根据已知的$p0$$p1$，可得，上述对数似然函数的最大化式等价于下面式子的最小化。$$\ell(\theta) = \sum_{i=1}^m(-y_i\theta^Tx_i + \ln (1+e^{\theta^Tx_i}))$$ 从另外一个角度，当我们令似然项$$p(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$ 可以同样的列出相应的最大似然函数，$$\begin{split}L(\theta) =&amp; \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta)\\=&amp;\prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}\end{split}$$ 同理的得其对数似然如下， $$\begin{split}\ell(\theta) &amp;= \log L(\theta) \\&amp;= \sum_{i=1}^m [y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log (1-h(x^{(i)})) ]\\&amp;=\sum_{i=1}^m [y^{(i)} \log \frac{h_\theta(x^{(i)})}{1-h_\theta(x^{(i)})} + \log(1-h_\theta(x^{(i)}))]\\&amp;=\sum_{i=1}^m[y_i\theta^Tx_i - \log(1+e^{\theta^Tx_i})]\end{split}$$ 即求上式的最大值。 因此，我们可以将代价函数用如下的式子来表示，（目的是要通过代价函数的最小值来估计出相应的参数）$$Cost(h_\theta,y) = -y\log(h_\theta(x)) - (1-y) \log(1-h_\theta(x))$$因此得到代价函数$J(\theta)$$${J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))\right]}$$后面的步骤就跟线性回归很相似了，可以直接用梯度下降法。$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac {1}{m} \sum _{i=1} ^m{(h_\theta(x^{(i)}) - y^{(i)})} x^{(i)}_j$$ 12345678%calculation Jz = X*theta;t = -y' * log(sigmoid(z)) - (1 - y') * log (1 - sigmoid(z));J = t / m; %calculation gradgrad_t = (sigmoid(z) - y);grad =( X' * grad_t )/ m; Multi-nominal logistic regression modelLogistic Regression只能用于二分类系统，对于一个多分类系统（K分类），我们假设离散变量$Y$的取值集合是{${1,2,3,\dots,K}$}，那么多项逻辑斯蒂回归模型是$$\begin {split}p(Y = k &amp;| x) = \frac{e^{\theta_{k}^{T}x}}{1+\sum_{k=1}^{K-1}e^{\theta_{k}^{T}x}}\\&amp;k = 1 ,2,3\dots,k-1\end{split}$$ $$p(Y = K| x) = \frac{1}{1+\sum_{k=1}^{K-1}e^{\theta_{k}^{T}x}}$$ 参考文献 机器学习 周志华著 统计学习方法 李航著 Couresera Machine Learning Andrew-Ng understanding-logistic-regression-using-odds Machine-Learning-Andrew-Ng-My-Notes logistic-regression 终于搞清楚什么是逻辑回归-对数几率回归]]></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小记 Machine Learning 系列 目录]]></title>
    <url>%2F2017%2F05%2F01%2FML-directory%2F</url>
    <content type="text"></content>
      <categories>
        <category>技术向</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>小记系列</tag>
        <tag>directory</tag>
      </tags>
  </entry>
</search>